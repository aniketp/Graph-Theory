\documentclass[11pt,a4paper]{extarticle}

\usepackage[top=1.5in]{geometry}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{titling}
\usepackage{changepage}
\usepackage{multicol}
\usepackage{listings}
\graphicspath{{images/}}

\setlength{\droptitle}{-10em}

\title{\textbf{Graph Theory \& Algorithms}}
%\date{23 December 2017}
\author{Aniket Pandey}

\begin{document}
\maketitle

\section{Introduction}
Graph Theory is the study of Graphs, the mathematical structures modelling the pairwise relation of Vertices (also called nodes) and Edges, where two nodes are connected by an edge. A graph can be directed or undirected, cyclic or acyclic, linear or weighted etc. 

Graph Theoretical concepts are widely used to study and model various applications, in different areas. For example, in Computer Science, problems like Travelling salesman problem, the Minimal spanning tree in a weighted graph and in Mathematics like Hamiltonian graphs, Fermat's Little Theorem \& Nielson-Schreier Theorem. 

\section{Notations}
\subsection{Big-O Notation}
Big-O Notations are used in mathematics to characterize functions according to their growth rate. In Graph Theory, efficiency of an algorithm is measured in terms of the input length $n$ as $n\rightarrow \infty $.\par Formal definition would be\\If $f:N\rightarrow N$ and $g:N\rightarrow N$ are two functions, then $f=$O$(g)$ if and only if $f(n)<c \cdot g(n)$ for a constant $c$ as $n\rightarrow\infty$.
\subsection{Other Notations}
There are a few more notations which complement Big-O Notation. I will give a brief information about these.\par
For functions $f$ \& $g$ from $N$ to $N$

\begin{align}
f =&\:\Omega(g)\quad  \textrm{if} \,\,  g=O(f)\\
f =&\:\Theta(g)\quad \textrm{if} \,\,  f=O(g) \,\, \& \,\, g=O(f)\\
f =&\:o(g)\:\,\quad \textrm{if there exists }\varepsilon \textrm{ such that} \,\, f(n)<\varepsilon\cdot g(n) \\
f =&\:\omega(g)\quad \:\textrm{if} \,\,  g=o(f)
\end{align}

\section{Terminologies}
Here are a few basic terminologies that are used to portray navigation through a Graph. \\
\begin{adjustwidth}{1.5em}{0pt}
\begin{description}
\item [Walk] A walk is any route through a graph from vertex to vertex along edges. It can end on the same vertex on which it began or on a different vertex and there is no limit on how many times a vertex is convered.
\item [Path] A path is a walk that does not include any vertex twice, except that its first vertex might be the same as its last.
\item [Cycle] A cycle is a path that begins and ends on the same vertex, like a closed loop.
\end{description}
\end{adjustwidth}

\section{Data Structures used in Graphs}
\subsection{Adjacency List}
An Adjacency List is a collection of unordered lists used to represent a finite graph. Each list describes the set of neighbors of a vertex in the graph.

\begin{center}
	\includegraphics[width=8cm, height=3cm]{list}
\end{center} 

\subsection{Adjacency Matrix}
An Adjacency Matrix is a square matrix used to represent a finite graph. The elements of the matrix indicate whether pairs of vertices are adjacent or not in the graph.

\[
    a(i,j)= 
\begin{cases}
    1,& \text{if } (i,j) \in E\\
    0,& \text{otherwise}
\end{cases}
\]

\begin{center}
	\includegraphics[width=8cm, height=3cm]{matrix}
\end{center}
 
\subsection{Stacks and Queues} 
Stacks and Queues are dynamic sets in which the element removed from the set by the \textit{delete} operation is prespecified. In a Stack, the element deleted from the set is the one inserted most recently, while in a Queue, the element that is to be deleted is the element which has been in the Queue for the longest time. \\

A \textbf{Stack} is a container of objects that are inserted and removed according to the last-in first-out (LIFO) principle. In the pushdown stacks only two operations are allowed, push the item into the stack and pop the item out of the stack.

A \textbf{Queue} is a container of objects that are inserted and removed according to the first-in first-out (FIFO) principle. In the queue only two operations are allowed, enqueue and dequeue. Enqueue means to insert an item into the back of the queue, dequeue means removing the front item.
\newpage
\begin{multicols}{2}

\begin{center}
	\textbf{STACK}
\end{center}

\lstinputlisting[language=Python, showstringspaces=false]{code/stack.py}

\columnbreak

\begin{center}
	\textbf{QUEUE}
\end{center}

\lstinputlisting[language=Python, showstringspaces=false]{code/queue.py}

\end{multicols}

\section{BFS and DFS}
\subsection{Breadth First Search}
Breadth First Search (or BFS) is a graph traversal algorithm through which we can determine the shortest path from a source node $V$ to any other node in the graph, which is visitable from $V$. BFS works by starting from the source node, then visits all its neighbours, assignes them the value $1$ (distance from $V$). It continues the process by visiting the neighbours of the current node which have not already been visited, assignes them the respective value.

Now the question is, how do we keep track of all visited nodes. Well, for that we use a \textit{boolean array} and a \textit{queue} for storing the status of each node and for storing a node's unvisited neighbours respectively. 

Here is a simple pseudocode of BFS. 

\begin{adjustwidth}{2.5em}{0pt}
\lstinputlisting[language=Python, showstringspaces=false]{code/bfs.py}
\end{adjustwidth}

\subparagraph{Explanation and Analysis}
In the beginning I have declared an empty set $visited$, and enqueued the source node $S$. Then as long as queue $Q$ is not empty (there are unvisited nodes), we'll dequeue it and check all its neighbours, if they have not been visited, enqueue them and mark them as visited. This process will continue till every node is in the visited array. We can further run a test in the boolean array to check if there is a node which cannot be visited from the source node.

For example, in the image shown below, the node 7 cannot be visited from the source node 3.

\begin{center}
	\includegraphics[width=8cm, height=3cm]{bfs}
\end{center}

\subparagraph{Time Complexity}
BFS loops through each Vertex and Edge to check the visited condition. So the complexity is,

\begin{align}
O(V,E) =& \,\,\,O(V+E)
\end{align}

\subsection{Depth First Search}

Depth First Search (or DFS) is a recursive Graph traversal algorithm using which we can determine the minimal spanning tree, presence of cycles and to check if the graph is bipartite. DFS works on the principle of backtracking, that is, it goes down a path, maintaining visited nodes in a \textit{stack} this time. When it can't find another unvisited node, it backtracks its path to the nearest node which has an unvisited neighbour. It continues this process until it lands back to the source node, implying that the traversal is complete.


\begin{adjustwidth}{2.5em}{0pt}
\lstinputlisting[language=Python, showstringspaces=false]{code/dfs.py}
\end{adjustwidth}

\subparagraph{Explanation and Analysis}
Apart from Graph and source node, the \textit{dfs} function also takes as an input an empty $stack$ which is used to keep track of visited chain of nodes, that is, we keep pushing onto the stack the nodes that have been visited, popping them once the recursion is complete. Then the part which checks for unvisited node is same as that in \textit{bfs}, only difference being we call dfs at every iteration.

\subparagraph{Time Complexity}
Same as that of BFS, the complexity of DFS is $O(V+E)$

\section{Single Source, Shortest Path}
Till now, both algorithms relied on the fact that the weight of each edge is unity. What if the weights were integers (even negative), in that case, the algorithms discussed above will not work. For instance in the images shown below

\begin{multicols}{2}

\begin{center}
	\includegraphics[width=5cm, height=3cm]{shortest}
\end{center}
Here, the direct path between 4 and 5 (58) is not the shortest one. 4-2-5 will be much shorter.

\columnbreak

\begin{center}
	\includegraphics[width=6cm, height=3cm]{negative}
\end{center}

Here, the cycle $BCD$ has negative length (-1) implying the cycle has infinite negative weight.

\end{multicols}

To alleviate this issue, I'll discuss two algorithms, namely \textit{Dijkstra's Algorithm} \& \textit{Bellman-Ford Algorithm}

\subsection{Dijkstra's Algorithm}
Dijkstraâ€™s algorithm finds a shortest path tree from a single source node, by building a set of nodes that have minimum distance from the source. The graph will have random connected nodes denoted by $u$ and $v$ and the weight of their edge will be $len(u,v)$. Purpose is to find the shortest path of every node from a given source node $s$.

\begin{adjustwidth}{2.5em}{0pt}
\lstinputlisting[language=Python, showstringspaces=false]{code/dijkstra.py}
\end{adjustwidth}

\subparagraph{Explanation and Analysis}
Dijkstra starts off by declaring an array \textit{dist} to store updated shortest distance of a node from the source. Clearly, $len(v,v) = 0$ and for every other node $u$, $dist[u] = \infty$. Now, we select the node which has the least distance from source (say $v$) and check if for a node $u$ $dist[v] + len(u,v) < dist[u]$. If so, then we update the value of $dist[u]$. This process continues till the no more updates can be done. (Shortest paths have been found)

\subparagraph{Time Complexity}
The code which evaluates vertex $v$ for which $dist[v]$ is minimum can be a binary search of complexity $O(logV)$. For every node $v$, there are $v-1$ nodes and hence, $v-1$ possible edges (routes). So the complexity for Dijkstra is,

\begin{align}
O(V,E) =& \,\,\,O(V) + O((E-1)logV) + O(c)\\
	   =& \,\,\,O(Elog(V))
\end{align}

\subsection{Bellman-Ford Algorithm}
The Bellman-Ford is another algorithm which calculates the shortest paths from a single source. It is slower than Dijkstra, but for appropriate reasons as it also takes into account negative weights. If we put a Graph with negative edge cycles into Dijkstra, it will ignore those cycles and give wrong results.

\begin{center}
	\includegraphics[width=6cm, height=3cm]{bellford}
\end{center}

\begin{adjustwidth}{2.5em}{0pt}
\lstinputlisting[language=Python, showstringspaces=false]{code/bell.py}
\end{adjustwidth}

\subparagraph{Explanation and Analysis}
Bellman-Ford works by overestimating the edge lengths of every node, assuming it to be a very large number. $node.prev = nil$ specifies the pointer to previous node which in this case has not yet been initialized. This algorithm relaxes the shortest path of each node by iterating over every possible edge, checks if $dist[v] > dist[u] + len(u,v)$ for adjacent nodes $(u,v)$ and if so, replaces the value of $dist[v]$. 

It has been proved that this iteration for $|V|$ times will make sure every shortest path is computed. Now, for the presence of any negative weight cycles, the last iteration again checks for triangular inequality for every pair $(u,v)$ in the Edge set. If it finds one, reports it as a negative weight cycle. Graph.\textit{prev}[] can be used to formulate the shortest path found out by this algorithm, by finding the predecessors of each node starting from the destination node.


\subparagraph{Time Complexity}
For various relaxations, Bellman-Ford checks $|E|$ edges for every node in $|V|$. So the time complexity is,

\begin{align}
O(V,E) =& \,\,\,O(V) + O(V.E) + O(E) + O(c)\\
	   =& \,\,\,O(V.E)
\end{align}

\section{Minimum Spanning Tree}
A \textbf{Spanning Tree} of a Graph is set of edges, essentially forming a tree such that all the vertices of the graph are connected and there is no formation of a cycle in the process. For a weighted graph, we can define a \textit{Minimum Spanning Tree} to be a spanning tree with the additional property that the sum of all weighted edges in minimum.

\begin{center}
	\includegraphics[width=6cm, height=3cm]{mst}
\end{center}

Mathmematically, we can define the MST problem as followes. Consider a Graph $G = (V,E)$, where $V$ and $E$ are usual notations for Set of all vertices and edges respectively. Since the Graph is weighed, for an edge $(u,v) \in E$, we have a weight function $w(u,v)$ specifying the distance between the edges. We wish to find an acyclic subset $T \subset E$ that connects all vertices and 
\begin{align}
w(T) =& \min{\sum_{(u,v) \in T} w(u,v)}
\end{align}

\noindent
gives the total length of the MST.

\subsection{Building a minimum spanning tree}
Algorithms to construct a Minimum Spanning Tree are greedy by nature. This can be proved by simple induction and the generic approach that is taken to build it. The two algorithms to grow an MST are \textbf{Kruksal's} and \textbf{Prim's} algorithm.

\section{Conclusion}
So these were the 4 major Graph Algorithms I studied for the project. After careful analysis, I realized the application of these algorithms is much more prevalent than the problems which they seem to elucidate. I was able to solve most of the problems concerning graphs by simply using the trivial BFS or DFS. However, on certain occasions, I had to go for the weighted giants. 

For the problems with application of either BFS or DFS, I had to observe how far I'll have to travel down the graph. For quick results, which are only concerned with examining a section of graph, I preferred DFS as its recursive nature allowed be to cover the relevant portions quickly. It also reduced the net complexity by not scanning every neighbour of every node.

Most of the problems involved application of these algorithms like topological sorting, checking for bipartite graph, finding shortest route, finding fastest route, detecting cycles and so on. I rarely encountered negative weights so never had to go for Bellman-Ford. Although I believe it will be useful in many advanced problems.

There are a few more renowned graph algorithms which I'd like to study later on, some of them being Floyd Warshall, Kruskal \& Prim's Algorithm. 

\end{document}
